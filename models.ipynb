{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install ctransformers\n",
        "!pip install langchain\n",
        "# !pip install weaviate-client\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "GLLtRI_NoOdn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "import json\n",
        "\n",
        "# auth_config = weaviate.AuthApiKey(api_key=\"YOUR-WEAVIATE-API-KEY\")\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url=\"https://genai-case-study-7d99r5vj.weaviate.network\",\n",
        "    # auth_client_secret=auth_config,\n",
        ")"
      ],
      "metadata": {
        "id": "9dl7dsCAKsCT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('is_ready:', client.is_ready())"
      ],
      "metadata": {
        "id": "f-gexLr2Kyf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "\n",
        "def generate_embeddings(text):\n",
        "  tokenizer = DPRContextEncoderTokenizer.from_pretrained(\n",
        "      \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "  )\n",
        "  model = DPRContextEncoder.from_pretrained(\n",
        "      \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "  )\n",
        "  input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "  embeddings = model(input_ids).pooler_output\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "Q6iZ8CQgLtoy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to install the air filter?\"\n",
        "query_vector = generate_embeddings(query)"
      ],
      "metadata": {
        "id": "Ss-w8-4kLj2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = client.query.get(\"DocumentSearch\", [\"source_text\"]).with_limit(5).with_near_vector({\n",
        "    \"vector\": query_vector,\n",
        "    \"certainty\": 0.7\n",
        "}).do()"
      ],
      "metadata": {
        "id": "xvCL8xhULqZx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = result['data']['Get'][\"DocumentSearch\"]"
      ],
      "metadata": {
        "id": "I8UHG4c7LrAQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in output:\n",
        "  try:\n",
        "    text = i[\"source_text\"]\n",
        "    i[\"source_text\"] = text.split(\" [META] \")[0]\n",
        "    i[\"start_index\"] = int(text.split(\" [META] \")[1].split(\":\")[1])\n",
        "    i[\"end_index\"] = i[\"start_index\"] + len(i[\"source_text\"])\n",
        "  except:\n",
        "    print(i)\n",
        "    break"
      ],
      "metadata": {
        "id": "ZNvzo2UBObHe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlap_merge(chunk1, chunk2):\n",
        "  \"\"\"\n",
        "  De-duplicate overlapping chunks.\n",
        "\n",
        "  Args:\n",
        "      chunk1 (str): first chunk or content.\n",
        "      chunk2 (str): second chunk or content.\n",
        "\n",
        "  Returns:\n",
        "      str: merged chunk.\n",
        "      int: status.\n",
        "  \"\"\"\n",
        "  if (chunk1['end_index']>chunk2['start_index']):\n",
        "      new_chunk = chunk1\n",
        "      new_chunk['source_text'] = chunk1['source_text'][:chunk1['end_index']] + chunk2['source_text'][chunk1['end_index']:]\n",
        "      new_chunk['end_index'] = chunk2['end_index']\n",
        "      return new_chunk, 1\n",
        "  else:\n",
        "      return chunk2, 0"
      ],
      "metadata": {
        "id": "pVEqxETKQzrW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "\n",
        "def chunk_dedup(top_k_docs):\n",
        "  \"\"\"\n",
        "  Check and De-duplicate overlapping chunks.\n",
        "\n",
        "  Args:\n",
        "      top_k_docs (tuple): top k retrieved documents.\n",
        "\n",
        "  Returns:\n",
        "      tuple: top k retrieved docs de-duplicated.\n",
        "  \"\"\"\n",
        "  chunk_list = sorted(top_k_docs, key=lambda x: x['start_index'])\n",
        "\n",
        "  final_chunk_list = []\n",
        "\n",
        "  for j in range(1,len(chunk_list)):\n",
        "      chunk_list[j],res = overlap_merge(chunk_list[j-1],chunk_list[j])\n",
        "\n",
        "      if res == 0:\n",
        "          final_chunk_list.append(chunk_list[j-1])\n",
        "\n",
        "  final_chunk_list.append(chunk_list[j])\n",
        "\n",
        "  return final_chunk_list"
      ],
      "metadata": {
        "id": "R8h9R2oMMNUn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\n",
        "for i in chunk_dedup(output):\n",
        "  context = context + i[\"source_text\"]"
      ],
      "metadata": {
        "id": "tdXcpe7hQIwR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a helpful chatbot use the following context to answer the question asked by the user. If you don't know the answer reply with \"I don't know\"\n",
        "Context: {}\n",
        "Question: {}\n",
        "Answer:\"\"\""
      ],
      "metadata": {
        "id": "YDGSs67PT_dz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = prompt.format(context, query)"
      ],
      "metadata": {
        "id": "f7-dacq6VIZi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "1MMU-GwTfi4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "    input,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text'].split('Answer:')[1]}\")\n"
      ],
      "metadata": {
        "id": "nGmtT2DGkk2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import CTransformers\n",
        "\n",
        "llm = CTransformers(\n",
        "            model=\"/content/drive/MyDrive/GenAI/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
        "            model_type=\"llama\",\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.5,\n",
        "            context_length = 2048\n",
        "        )"
      ],
      "metadata": {
        "id": "3L3tLz0MhFRd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(input)"
      ],
      "metadata": {
        "id": "jO79QBHkkipT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zgc7Jn1sw1bN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}