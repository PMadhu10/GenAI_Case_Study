{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install frontend\n",
        "!pip install pytesseract\n",
        "!pip install langchain\n",
        "!pip install sentence-transformers\n",
        "!pip install weaviate-client\n",
        "!sudo apt-get install tesseract-ocr\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "eE94AeURiNX7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction"
      ],
      "metadata": {
        "id": "oauo-4ujlvFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VnoStjWVhgSh"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import io\n",
        "import tempfile\n",
        "import cv2\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def save_temp_image(image, img_index, pgno, rotated = False):\n",
        "    # Save the PIL Image to a temporary file\n",
        "    temp_dir = os.getcwd() + \"/temp\"\n",
        "    if not rotated:\n",
        "        temp_image_path = os.path.join(temp_dir, f\"temp_image_{pgno}_{img_index}.jpg\")\n",
        "    else:\n",
        "        temp_image_path = os.path.join(temp_dir, f\"temp_image_rotated_{pgno}_{img_index}.jpg\")\n",
        "    image.save(temp_image_path)\n",
        "    return temp_image_path\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    extracted_text = \"\"\n",
        "\n",
        "    for page_number in tqdm(range(pdf_document.page_count)):\n",
        "        # Get the page\n",
        "        page = pdf_document[page_number]\n",
        "\n",
        "        # Extract text from the PDF page\n",
        "        pdf_text = page.get_text(\"text\")\n",
        "        extracted_text += f\"Page {page_number + 1} PDF Text:\\n{pdf_text}\\n{'-' * 50}\\n\"\n",
        "\n",
        "        # Get the images on the page\n",
        "        images = page.get_images(full=True)\n",
        "\n",
        "        for img_index, img_info in enumerate(images):\n",
        "            # Get the image\n",
        "            img_index += 1\n",
        "            base_image = pdf_document.extract_image(img_info[0])\n",
        "\n",
        "            # Check if image extraction is successful\n",
        "            if not base_image:\n",
        "                print(f\"Failed to extract image {img_index} on page {page_number + 1}\")\n",
        "                continue\n",
        "\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Convert image bytes to PIL Image\n",
        "            with io.BytesIO(image_bytes) as image_buffer:\n",
        "                pil_image = Image.open(image_buffer)\n",
        "\n",
        "                temp_image_path = save_temp_image(pil_image, img_index, page_number)\n",
        "\n",
        "                # Check for image orientation and rotate if needed\n",
        "                try:\n",
        "                  orientation, lang = get_image_orientation(temp_image_path)\n",
        "                  print(temp_image_path, orientation, lang)\n",
        "                  pil_image = pil_image.rotate(orientation, expand=True)\n",
        "                except:\n",
        "                  pass\n",
        "\n",
        "                # dpi = pil_image.info['dpi']\n",
        "\n",
        "                # custom_config = r'--dpi ' + str(dpi)\n",
        "\n",
        "                # Perform OCR on the image\n",
        "                image_text = pytesseract.image_to_string(pil_image, lang='eng')  # , config = custom_config)\n",
        "\n",
        "                # Add image text to the overall extracted text\n",
        "                extracted_text += f\"Page {page_number + 1}, Image {img_index} Text:\\n{image_text}\\n{'-' * 50}\\n\"\n",
        "\n",
        "                # Clean up: Remove the temporary image file\n",
        "            os.remove(temp_image_path)\n",
        "\n",
        "    # Close the PDF document\n",
        "    pdf_document.close()\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "def get_image_orientation(image_path):\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    osd_data = pytesseract.image_to_osd(img)\n",
        "\n",
        "    # Extract orientation and language from the OSD data\n",
        "    lines = osd_data.splitlines()\n",
        "    orientation = int([line for line in lines if 'degrees' in line][0].split(': ')[1])\n",
        "    lang = str([line for line in lines if 'Script' in line][0].split(': ')[1])\n",
        "\n",
        "    return orientation, lang\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(pdf_path):\n",
        "  start_time = time.time()\n",
        "  # pdf_path = \"/content/drive/MyDrive/GenAI/PDF Documents/Set1/Black_Decker_AirCompresssor_Nil1_Editable.pdf\"\n",
        "  combined_text = extract_text_from_pdf(pdf_path)\n",
        "  end_time = time.time()\n",
        "  print(\"Time to extract \", end_time - start_time)\n",
        "  output_file = os.getcwd() + '/output/output.txt'\n",
        "\n",
        "  return combined_text\n",
        "\n",
        "# Print or use the combined extracted text as needed\n",
        "# with open(output_file, 'w', encoding='utf-8') as file:\n",
        "#         file.write(combined_text)"
      ],
      "metadata": {
        "id": "oKPDI9laiL36"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "6t4f8NM9l_Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "PCHjtyoYmCyr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(combined_text):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "              chunk_size=500,\n",
        "              chunk_overlap=50,\n",
        "              add_start_index= True,\n",
        "              length_function= len\n",
        "          )\n",
        "  # Step 3: Split the content into chunks\n",
        "  texts = text_splitter.create_documents([combined_text])\n",
        "  return texts\n",
        "\n",
        "# print(len(texts))"
      ],
      "metadata": {
        "id": "BkARwY_Zm41q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n"
      ],
      "metadata": {
        "id": "PAQKR9JZrUY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_extract(text,meta_dict):\n",
        "  text += \" [META]\"\n",
        "  for k,v in meta_dict.items():\n",
        "    text += \" \" + k + \":\" + str(v)\n",
        "  return text"
      ],
      "metadata": {
        "id": "Av1wWGKZF7RD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metadata(texts):\n",
        "  for i in tqdm(range(len(texts))):\n",
        "    texts[i] = metadata_extract(texts[i].page_content, texts[i].metadata)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "G0PPE3mk1c92"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "\n",
        "def generate_embeddings(text):\n",
        "  tokenizer = DPRContextEncoderTokenizer.from_pretrained(\n",
        "      \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "  )\n",
        "  model = DPRContextEncoder.from_pretrained(\n",
        "      \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "  )\n",
        "  input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "  embeddings = model(input_ids).pooler_output\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "0_v9oLH7rW8C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "import json\n",
        "\n",
        "# auth_config = weaviate.AuthApiKey(api_key=\"YOUR-WEAVIATE-API-KEY\")\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url=\"https://genai-case-study-7d99r5vj.weaviate.network\",\n",
        "    # auth_client_secret=auth_config,\n",
        ")"
      ],
      "metadata": {
        "id": "G9lyCVzSInzK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('is_ready:', client.is_ready())"
      ],
      "metadata": {
        "id": "Uo6onKnIFq1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_info = client.get_meta()\n",
        "print(json.dumps(meta_info, indent=2))"
      ],
      "metadata": {
        "id": "imnZbQ4SFrrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_obj = {\"class\": \"DocumentSearch\", \"vectorizer\": \"none\"}\n",
        "client.schema.create_class(class_obj)"
      ],
      "metadata": {
        "id": "DtKgBV_yF2XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_texts(texts):\n",
        "  client.batch.configure(batch_size=len(texts))\n",
        "  with client.batch as batch:\n",
        "      for i, doc in enumerate(texts):\n",
        "          properties = {\"source_text\": doc}\n",
        "          vector = generate_embeddings(doc)\n",
        "          batch.add_data_object(properties, \"DocumentSearch\", vector=vector)"
      ],
      "metadata": {
        "id": "obFXduPAF-dP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/GenAI/PDF Documents/Set2/\"\n",
        "for file in tqdm(os.listdir(folder_path)):\n",
        "  file_path = os.path.join(folder_path,file)\n",
        "  combined_text = extract_text(file_path)\n",
        "  texts = split_text(combined_text)\n",
        "  texts = get_metadata(texts)\n",
        "  upload_texts(texts)"
      ],
      "metadata": {
        "id": "c6kPVV7YIPPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to install the air filter?\"\n",
        "query_vector = generate_embeddings(query)"
      ],
      "metadata": {
        "id": "6x1k9jMwHYiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = client.query.get(\"DocumentSearch\", [\"source_text\"]).with_limit(2).with_near_vector({\n",
        "    \"vector\": query_vector,\n",
        "    \"certainty\": 0.7\n",
        "}).do()"
      ],
      "metadata": {
        "id": "DjdvJmYgKsXT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['data']['Get'][\"DocumentSearch\"]"
      ],
      "metadata": {
        "id": "G9tDtL_QKn3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8ikZTeuKuOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}